 •Leveraging Transformer Models for Multilingual Hierarchical Classification of Articles
 Nov- Jan, 2024
 • A Study of Unified mBERT and XLM-R with Attention Networks
 • Developed and evaluated a multilingual hierarchical classification model using XLM-R and transformer-based attention networks to improve narrative and sub-narrative labeling in multilingual news articles, addressing challenges in class imbalance and multilingual adaptability.
